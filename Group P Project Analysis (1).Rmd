---
title: 'Group P: Project Analysis Component'
author: Sydney Bluestein, Parker Smith, Tate Eppinger, and Juan Perez
output: pdf_document
date: "2024-04-03"
---
```{r include=FALSE}
library(readr)
library(mosaic)
library(mosaicData)
library(GGally)
library(Stat2Data)
library(leaps)
library(broom)
library(rvest)
library(methods)
require("gridExtra")
Sport_car_price <- read_csv("~/Desktop/STAT230/Sport car price.csv")
```
```{r warning=FALSE}
Cars <- janitor::clean_names(Sport_car_price)
Cars <- unique(Cars)
Cars <- Cars %>% 
  filter(!grepl("Electric", engine_size_l)) %>% 
  mutate(across(engine_size_l:x0_60_mph_time_seconds, as.numeric))
Cars2<-Cars[-c(94, 150, 154, 409),] #deletes middle eastern cars and super old car
Cars3<-filter(Cars2, price_in_usd<500000) #removed data from IQR test below
Cars4<-na.omit(Cars3)
## MAR: Recategorized into "Asian", "Domestic", and "European"
SportsCars <- Cars4 %>% mutate(car_make = case_when(
    car_make %in% c("Acura", "Kia"  , "Lexus"  , "Mazda"  , "Nissan" , "Subaru" , "Toyota") ~ "Asian",
    car_make %in% c("Ford" , "Chevrolet" , "Dodge") ~ "Domestic",
    car_make %in% c("Alfa Romeo" , "Alpine" , "Ariel" , "Aston Martin" , "Audi" , "Bentley", "BMW", "Bugatti", "Ferrari", "Jaguar", "Koenigsegg", "Lamborghini", "Lotus", "Maserati", "McLaren", "Mercedes-AMG", "Mercedes-Benz", "Pagani", "Polestar", "Porsche", "Rimac", "Rolls-Royce", "TVR", "Ultima" ) ~ "European"))
```
Does “what’s under the hood” matter when predicting sports car prices in America? vroom vroom
Group P: Sydney Bluestein, Tate Eppinger, Juan Perez, and Parker Smith

Project Aim: We want to explore which factors contribute to sports car prices in America. We have chosen numerous explanatory variables that may influence sports car prices: car origin, car year, 0-60 speed, torque, horsepower, and engine size. 

Research Question: Do any of the factors above (or a combination of them) predict sports car prices accurately? 

Variables: Car origin (Asian, Domestic, or European) is our categorical variable. Year is a quantitative variable that represents the year the car was made. 0-60 speed is a quantitative variable that represents the time in seconds it takes for the car to go from 0 miles per hour to 60 miles per hour. Torque is a quantitative variable measured in pounds-feet that describes the power of the car’s drivetrain. Horsepower is another quantitative variable measured in foot-pounds/minute representing engine power. Lastly, engine size is a quantitative variable in liters that measures how much gas the car can hold. 

Step 1 – Univariate Analysis: 
We included a brief univariate analysis in our project proposal. We included density plots of horsepower, torque, engine size, and 0-60. It would be useful to obtain descriptive stats for these quantitative variables using the favstats method. Looking into the quantitative variables’ shape, center, and spread will help orient us toward recognizing typical and unusual values for variables measured in unfamiliar units. Car year is characterized by a box plot. We removed a few cars produced before 2014 since we are attempting to predict sports car prices from the last decade. We also included a bar graph for our categorical variable, car origin. We notice that European cars are most commonly represented in this data set. However, there are still enough Asian and domestic sports cars to make this variable worthwhile. We will use the tally function to create a table illustrating the exact distribution of cars originating from Europe, Domestic, and Asia.
```{r warning=FALSE}
m1<-gf_dens(~ horsepower, data = SportsCars, color = "blue")        
m2<-gf_dens(~ torque_lb_ft, data = SportsCars, color = "deepskyblue")
m3<-gf_dens(~ engine_size_l, data = SportsCars, color = "pink2")
m4<-gf_dens(~ x0_60_mph_time_seconds, data = SportsCars,  xlab="0 to 60mph time in seconds", color = "purple")
```
```{r warning=FALSE}
m5<-gf_bar (~car_make, data= SportsCars)
m6<-gf_boxplot(~year, data=SportsCars)
favstats(~year, data=SportsCars) #IQR test 1.5, 1.5(IQR=1)=1.5, data range from 2019 to 2023
grid.arrange(m1, m2, m3, m4, m5, m6, ncol=3, heights=c(2, 2, 2))
```
Step 2 – Bivariate Analysis: 
We will begin with simple linear regression to gauge which explanatory variables help predict sports car prices. It will be time-efficient to begin with GGpairs as a summary of the relationships between our explanatory variables and sports car prices. Upon the first glimpse, the GGpairs plot demonstrates that horsepower (r=0.648) and 0-60 (r=-0.554) may be helpful predictors for sports car prices. We plan to explore this relationship further by creating simple linear regression models. To check for linearity, we can look either at the SLR plot or the residuals vs. fitted plot to ensure the relationship between our predictor variable and sports car price is best described as linear. Independence is not a condition we can assume from a plot. (We have very little information regarding the origin of this dataset, so we will have to proceed with caution). Looking at whether the data points fit well on the expected line QQ plot line will allow us to decide whether the normality condition is satisfied. We will confirm homoscedasticity for the equal variance condition by looking for a seemingly random distribution of points on the residuals vs. fitted plot. If we encounter any issues with conditions, we will look into removing potential outliers or transforming the data if necessary. 

```{r}
ggpairs(SportsCars, columns = c(3:8))
```

Step 3 – Model Building and Variable Selection: Once we have looked into SLR, we will turn to multiple linear regression.  Looking at the GGpairs plot, we notice many potential pairs of predictors run the risk of being collinear. (torque and horsepower, 0-60 and horsepower, and horsepower and engine size). We can run a VIF test if we choose to include variables that may be collinear to confirm we have created a stable model. We may use an automated variable selection method (maybe stepwise, so we get the best predictors at every stage) to delineate which combination of explanatory variables will help predict sports car prices best. We will then weigh the models’ parsimony with adjusted R-squared values and Mallow’s Cp to determine the best model(s).  If we have a potential outlier, we can use a Cook’s distance plot to determine if the point is unusual enough to remove from our model.

Regarding our categorical variable, we would like to run an ANOVA test to determine if the mean sports car price differs based on its region of origin. Suppose the ANOVA test is associated with a significant F statistic. We will then run a Tukey test (or a similar multiple comparison test) to determine which region(s) of origin have significantly different sports car prices. The results from this multiple comparison test will help us determine how we need to adjust our model, i.e., for what region of origin do we need to account for higher sports car prices relative to the other regions? Perhaps we would want to look into some interaction terms as well. 


SLR:
Horsepower and 0-60 appeared to be our strongest predictors for car price. However the scatterplot of horsepower vs. car price proved to be pretty curvy. I attempted to transform the data by predicting log (car price) rather than car price, log(horsepower) instead of horsepower, etc. However, conditions still were not met. Similarly for 0-60, conditions were not met for SLR. I removed an outlier, representing a car with extremely high torque and horsepower. I believed this point was pulling the regression line down. After removing this point, I then transformed the data by having -1/sqrt(0-60) predict price. However, conditions for SLR still failed to be met. I then wanted to check if using horsepower and 0-60 together would provide a better fitting model than the predictors on their own. Most of the points in the fitted vs. residuals plot are shifted to the right, demonstrating that there is not perfectly equal variance. Also the QQ plot appears curvy, however it looks less sigmoidal than any of the SLR model QQ plots. Though conditions are still not met perfectly, this model fits better than either of the SLR models. We will consider this bivariate model with caution.

```{r}

gf_point(price_in_usd ~horsepower, data=SportsCars) %>% gf_lm()
horsepowerSLR<- lm(price_in_usd ~horsepower, data=SportsCars) 
msummary(horsepowerSLR)
mplot(horsepowerSLR, which=1)
mplot(horsepowerSLR, which=2)
SportsCars <- mutate(SportsCars, horsepowerlog = log(horsepower)) 
gf_point(price_in_usd ~horsepowerlog, data=SportsCars) %>% gf_lm()
horsepowerSLR3<- lm(price_in_usd ~horsepowerlog, data=SportsCars) 
msummary(horsepowerSLR3)
mplot(horsepowerSLR3, which=1)
mplot(horsepowerSLR3, which=2)
SportsCars <- mutate(SportsCars, logprice = log(price_in_usd))
horsepowerSLR2<- lm(logprice ~horsepower, data=SportsCars) 
msummary(horsepowerSLR2)
mplot(horsepowerSLR2, which=1)
mplot(horsepowerSLR2, which=2)
```


```{r}
SportsCars <- filter(SportsCars, torque_lb_ft<1299)
SportsCars <- mutate(SportsCars, logprice = log(price_in_usd)) 
gf_point(logprice ~horsepower, data=SportsCars) %>%
gf_lm()

```


```{r}
aovmultiplecars<- lm(price_in_usd ~x0_60_mph_time_seconds + horsepower, data=SportsCars) 
msummary(aovmultiplecars)
mplot(aovmultiplecars, which=1)
mplot(aovmultiplecars, which=2)
SportsCars<- mutate(SportsCars, logprice= log(price_in_usd))
aovbivariate2<-lm(logprice ~x0_60_mph_time_seconds + horsepower, data=SportsCars) 
msummary(aovbivariate2)
mplot(aovbivariate2, which=1)
mplot(aovbivariate2, which=2)
```


```{r}
gf_point(price_in_usd ~x0_60_mph_time_seconds, data=SportsCars) %>% gf_lm()
zerotosixtySLR<- lm(price_in_usd ~x0_60_mph_time_seconds, data=SportsCars) 
msummary(zerotosixtySLR)
mplot(zerotosixtySLR, which=1)
mplot(zerotosixtySLR, which=2)
SportsCars <- mutate(SportsCars, zerotosixtyspecial = -1/sqrt(x0_60_mph_time_seconds)) 
gf_point(price_in_usd ~ zerotosixtyspecial, data=SportsCars) %>%
gf_lm()
zerotosixtySLR3<- lm(price_in_usd ~ zerotosixtyspecial, data=SportsCars) 
msummary(zerotosixtySLR3)
mplot(zerotosixtySLR3, which=1)
mplot(zerotosixtySLR3, which=2)
```



```{r}
kitchen1<- lm(price_in_usd ~ year + engine_size_l + horsepower + torque_lb_ft + x0_60_mph_time_seconds, data=SportsCars) 
msummary(kitchen1) 

car::vif(kitchen1) #x_60_mph_time_seconds not significant 
```


```{r}
stepwise <- regsubsets(price_in_usd ~ year + engine_size_l + horsepower + x0_60_mph_time_seconds, data = SportsCars, method = "seqrep", nbest = 1)
with(summary(stepwise), data.frame(cp, outmat)) #scrapped torque_lb_ft because of high correlation with horsepower 

MLR4<- lm(price_in_usd ~ year + engine_size_l + horsepower, data=SportsCars)
msummary(MLR4)
car::vif(MLR4) #best model according to stepwise
```

Next, we will attempt to run an ANOVA with the factor car_make, or the car region of origin. The results will allow us to determine if the geographical region of production affects the price enough to add it to a potential multivariate model. This factor has three categories: Asian, European, and Domestic. To get a better sense of this categorical variable, we compared box plots of each category. Immediately, we see that the European cars have a much higher standard deviation than both other categories, but especially domestic cars. Although this could be due to smaller sample sizes of Domestic and Asian car brands, in order to compare the categories at all, we need to check the requisite conditions to run an ANOVA.

```{r}
gf_boxplot(price_in_usd ~ car_make, data = SportsCars)
location <- lm(price_in_usd ~ car_make, data = SportsCars)
favstats(price_in_usd ~ car_make, data = SportsCars)
```

We then checked the conditions to run an ANOVA. The condition of equal variance is clearly not passed, due to the largest over smallest standard deviation being well over 2 (94260.05/18167.74 = 5.188) and the obvious lack of continuity on the Residuals vs. Fitted plot. Furthermore, the condition of normality is not passed due to the tails of the QQ plot clearly seperating from the line.

```{r}
mplot(location, which = 1)
mplot(location, which = 2)
```

Since we did not pass conditions and still want to run a comparison test on this variable, the next step of action is to attempt a transformation. We attempted to take the natural log of the response variable, price. Although this helped increase our equal variance and pass the standard deviation test (0.579/0.308 = 1.880), we still see a clear lack of normality in the QQ plot.

```{r}
gf_boxplot(logprice ~ car_make, data = SportsCars)
loglocation <- lm(logprice ~ car_make, data = SportsCars)
favstats(logprice ~ car_make, data = SportsCars)
mplot(loglocation, which = 1)
mplot(loglocation, which = 2)
```

Since we could not find a transformation that allowed us to pass conditions for running an ANOVA, we next moved towards running a randomization F-test. This test can help us compare categories across a response variable even when we do not pass conditions for an ANOVA. First, we randomly shuffled the price 10000 times in order to compare the frequency at which a random shuffle (which should have no main effects) will take on the same F-value as we observe in this data set.

```{r}
set.seed(40)
randomizedsamples <- do(10000) * (anova(lm(shuffle(price_in_usd) ~ car_make, data = SportsCars))$"F value")
randomizedsamples <- as.data.frame(randomizedsamples)
```

We then calculated the baseline F value, which is 77.48.

```{r}
anova(location)
origres <- anova(location)$"F value"#
origres
```

We then will observe how often our randomly sampled sets end up with an F value close to the 77.48 we see in our original data set. We observe that the random samples do not come close to obtaining an F value that high. In fact, we see almost no F-values over 5.

```{r}
gf_dens(~ V1, data = randomizedsamples) %>%
  gf_lims(x = c(0, 25)) %>%
  gf_labs(title = "Randomization Distribution for Car Make from ANOVA")
```

Next, we see the probability of getting an F like ours when the null (sports cars produced everywhere have the same price) is true. This value is 0, indicating that we should reject the null. We have evidence, using F-randomization, that there is a significant difference in the average price of a sports car between at least one of these three geographical regions.

```{r}
pdata(~ V1, origres[1], data = randomizedsamples, lower.tail = FALSE)
```

Since we got a significant result from the randomization F-test, we then will run a post-Hoc test to determine which categories are significantly different. We ran Tukey's HSD and found evidence that there are significant differences in the average price of sports cars that are produced in Europe vs. Asia (p < 0.001) and significant differences in the average price of cars produced domestically vs. those produced in Europe (p < 0.001).

```{r}
TukeyHSD(location)
```

Since there is evidence that European cars are significantly more expensive than Asian or Domestic makes, we will attempt to add an indicator variable to our best multiple linear regression model to determine if it reduces the error. By indicating all European cars with a "1" and all others with a "0", we can add the variable to our model. 

```{r}
IndSportsCars <- SportsCars %>% mutate(car_make = case_when(
    car_make %in% c("Asian") ~ 0,
    car_make %in% c("Domestic") ~ 0,
    car_make %in% c("European") ~ 1))
